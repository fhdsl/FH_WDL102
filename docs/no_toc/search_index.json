[["index.html", "WDL102: Designing, Testing and Optimizing Workflows About this Guide", " WDL102: Designing, Testing and Optimizing Workflows December 28, 2022 About this Guide This guide is intended to be an introduction to designing and testing WDL workflows for beginner and intermediate users who primarily develop and run their own workflows for their own research work. "],["introduction.html", "Chapter 1 Introduction 1.1 What is WDL? 1.2 What Is a WDL Engine? 1.3 Why use WDL for my research?", " Chapter 1 Introduction This guide is an introduction to designing and testing WDL workflows for beginner and intermediate users who primarily develop and run their own workflows for their own research work. 1.1 What is WDL? The Workflow Description Language (WDL) is a way to specify data processing workflows with a human-readable and -writeable syntax. WDL makes it straightforward to define analysis tasks, chain them together in workflows, and parallelize their execution. The language makes common patterns simple to express, while also admitting uncommon or complicated behavior; and strives to achieve portability not only across execution platforms, but also different types of users. Whether one is an analyst, a programmer, an operator of a production system, or any other sort of user, WDL should be accessible and understandable. 1.1.1 OpenWDL WDL was originally developed for genome analysis pipelines by the Broad Institute. As its community grew, both end users as well as other organizations using WDL for their own software, it became clear that there was a need to allow WDL to become a true community driven standard. The OpenWDL community has thus been formed to steward the WDL language specification and advocate its adoption. There is ongoing work on WDL the specification, thus it has multiple versions. Currently there are three versions to note: - draft-2 - this version was the version that much of the Broad’s public facing documentation and example workflows were written in. - 1.0 - this is a more recent version that is the most up to date version of the specification that Cromwell can interpret. We use WDL 1.0 at the Hutch when we use Cromwell. - 1.1 - this is an even more recent version that not all WDL engines support yet. We’ll be using WDL 1.0 in this course but you can always check out the openwdl repo for more information about tweaking these instructions for different versions of WDL. 1.1.2 Documentation about the WDL Spec To begin learning how to write WDL itself, the best (and evolving) resource is the WDL-Docs site being developed by the OpenWDL community. There you’ll find examples and guidance about the 1.0 WDL specification and using it to write workflows. 1.1.3 Tools to Edit WDL VSCode has multiple extensions for WDL, including “WDL DevTools” and “WDL Syntax Highlighter” which can be very helpful for color-coding content and detecting issues while you’re writing, before workflow validation. Also, there are a variety of extensions that can make working with the input files in json format much easier. They can be used to detect errors, color code them, format them in ways more easy to view as a human. Some examples include, json, Prettify JSON, or JSON (Beautify JSON). 1.2 What Is a WDL Engine? A WDL engine is software that understands WDL and can interpret the instructions in order to execute the computational tasks involved in the workflow. An example of a WDL engine, one used at the Fred Hutch, is Cromwell. Cromwell is a software tool that can be used to coordinate and streamline the actual running of bioinformatic (or other analytic) workflows. It is the software that underlies the Broad Institute’s Terra platform. It’s one of several tools and platforms that are able to run workflows written in the WDL language, including miniWDLfrom the Chan-Zuckerburg initiative, DNANexus, and other emerging tools. 1.3 Why use WDL for my research? 1.3.1 Abstracting Storage and Computing Details At many research institutions we have many resources for data storage, software installations and high performance computing available to researchers, which themselves are evolving. Using a workflow manager that is configured specifically to the types of data storages available at an institution can provide the benefits of “abstracting” or hiding some degree of the logistics involved with actually accessing those data for use in an analysis from the user. For instance, beyond data storage in a local file system, researchers now may have the ability to store data in AWS S3 storage. Accessing those data for the purposes of using them in workflows requires slightly different knowledge and mechanisms of retrieval (e.g. knowledge of how to use the AWS CLI). By providing one configuration (or set of instructions for accessing data) for Cromwell that is tailored to what is available at the institution, any individual researcher can use Cromwell as an interface to their data, wherever they may be stored. This intermediate tool then reduces the amount of time and effort on the part of the researcher in understanding the data storage flavor of the day, allowing them to focus more on their data and their analyses. An even more valuable benefit exists for high performance computing resources. When Cromwell is used as an intermediary, the backend configurations (or instructions for how to send jobs to HPC resources) can be defined once for all those resources currently available to Hutch users (such as our local SLURM cluster or AWS Batch for cloud computing). This means that individual users do not have to become SLURM or AWS Batch experts in order to be able to run jobs on them. They instead can focus on building their workflow and analyzing their results. 1.3.2 Reproducibility and Portability Beyond the benefits of abstracting some of the institution-specific details from the analysis itself, this creates a convenient side effect. The lack of need to tailor a workflow itself to the unique combination of data storage and computing resources, means that researchers focus their time on developing workflows using the workflow language WDL. WDL is an open source workflow description language that one can use to describe the inputs, environments and outputs of individual tasks and how they are strung together for an analysis. By creating workflows using tasks with these features defined, it makes these workflows far more reproducible (e.g. running a given workflow on any compute infrastructure will create results that are identical), and far more portable. In academia where collaborations and career transitions mean potential loss of time spent tailoring analyses to various institutions’ infrastructure is the name of the game, this benefit holds substantial value. 1.3.3 Software and Environments One challenge associated with reproducibility and portability that is something often undervalued in many bioinformatic analyses, is the management of computing software and environments. As projects evolve, they often begin in a testing phase with small datasets using readily available software or packages on a desktop computer or even potentially on a shared, remote computing resource. As they scale, it often is natural to simply do X, but Y times, all using the same computing environment of the user who originally developed the workflow. While this is common, it also creates headaches later. Using a workflow manager like Cromwell as a tool for deploying your analyses also means that you can begin your work directly by developing a WDL and running the tests via Cromwell. As you develop your workflow and want to scale it to an entire analysis, you can simply edit your workflow and Cromwell will keep track of all the tasks and their interrelations. While workflows are being developed in the context of a WDL for running with Cromwell (or other WDL runners that are being developed), each task must be defined to occur in a specific environment. That could be a user’s environment on the local SLURM cluster, or that could be a clean environment with specific software packages loaded, or it could be a docker or singularity container that directly specifies the software and environment in which to run the task. "],["designing-wdls.html", "Chapter 2 Designing WDLs 2.1 Writing Workflows 2.2 Customizing Workflow Runs", " Chapter 2 Designing WDLs The “Getting Started with WDL” and “WDL Script Components” sections of the OpenWDL docs site provides a useful background into the essential parts of WDL you’ll need to learn to start designing your own WDLs. A very useful way to begin learning how to write a WDL is actually to take a WDL that you know “works” in that it has the required formatting and structure to be executed by a WDL engine, and edit it. We have created a repository with test workflows in it that can serve as a basis to get started. 2.1 Writing Workflows WDL is intended to be a relatively easy to read workflow language. There are other workflow languages and workflow managers that run them, but WDL was/is primarily intended to be shared and interpreted by others that may or may not have experience with any given domain-specific language (like Nextflow). Writing workflows in WDL, therefore, has a fairly low barrier to entry for those of us new to writing workflows. The basics of this workflow language includes a similar pattern where you specify workflow inputs, the order of tasks and how they pass data between them, and which files are the outputs of the workflow itself. Here is an example of a single-task workflow that takes some inputs, runs bwa mem, then outputs the resulting bam file and it’s index. version 1.0 workflow myWorkflowName { input { String sample_name = &quot;sample1&quot; File myfastq = &quot;/path/to/myfastq.fastq.gz&quot; } call BwaMem { input: input_fastq = myfastq, base_file_name = sample_name, ref_fasta = &quot;hg19.fasta.fa&quot; ref_fasta_index = &quot;hg19.fasta.fa&quot;, ref_dict = &quot;hg19.fasta.dict&quot;, ref_alt = &quot;hg19.fasta.alt&quot;, ref_amb = &quot;hg19.fasta.amb&quot;, ref_ann = &quot;hg19.fasta.ann&quot;, ref_bwt = &quot;hg19.fasta.bwt&quot;, ref_pac = &quot;hg19.fasta.pac&quot;, ref_sa = &quot;hg19.fasta.sa&quot; } output { File output_bam = BwaMem.output_bam File output_bai = BwaMem.output_bai } } Tasks are constructed in the same WDL file by naming them and providing similar information. Here’s an example of a task that runs bwa mem on an interleaved fastq file using a Fred Hutch docker container. Notice there are sections for the relevant portions of the task, such as input (what files or variables are needed), command (or what it should run), output (which of the generated files is considered the output of the task), and runtime (such as what HPC resources and software should be used for this task). task BwaMem { input { File input_fastq String base_file_name File ref_fasta File ref_fasta_index File ref_dict File ref_alt File ref_amb File ref_ann File ref_bwt File ref_pac File ref_sa } command { bwa mem \\ -p -v 3 -t 16 -M \\ ${ref_fasta} ${input_fastq} &gt; ${base_file_name}.sam samtools view -1bS -@ 15 -o ${base_file_name}.aligned.bam ${base_file_name}.sam } output { File output_bam = &quot;${base_file_name}.aligned.bam&quot; File output_bai = &quot;${base_file_name}.aligned.bam.bai&quot; } runtime { memory: &quot;32 GB&quot; cpu: 16 docker: &quot;fredhutch/bwa:0.7.17&quot; } } You can find more about constructing the rest of your workflow at the OpenWDL docs site, and in future content here. 2.1.1 Design Recommendations In order to improve shareability and also leverage the fh.wdlR package, we recommend you structure your WDL based workflows with the following input files: Workflow Description file in WDL format, a list of tools to be run in a sequence, likely several, otherwise using a workflow manager is not the right approach. This file describes the process that is desired to occur every time the workflow is run. Parameters file in json format, a workflow-specific list of inputs and parameters that are intended to be set for every group of workflow executions. Examples of what this input may include would be which genome to map to, reference data files to use, what environment modules to use, etc. Batch file in json format, is a list of data locations and any other sample/job-specific information the workflow needs. Ideally this would be relatively minimal so that the consistency of the analysis between input data sets are as similar as possible to leverage the strengths of a reproducible workflow. This file would be different for each batch or run of a workflow. Workflow options (OPTIONAL) A json that contains information about how the workflow should be run (described below). 2.2 Customizing Workflow Runs You can tailor how a workflow might be run by various backends (computing infrastructure like a SLURM cluster or a cloud based compute provider), or with customized defaults. 2.2.1 Workflow Options You can find additional documentation on the Workflow Options json file that can be used to customize how Cromwell runs your workflow. We’ll highlight some specific features that are often useful for many users. Workflow options can be applied to any workflow to tune how the individual instance of the workflow should behave. There are more options than these that can be found in the Cromwell docs site, but of most interest are the following parameters: workflow_failure_mode: NoNewCalls indicates that if one task fails, no new calls will be sent and all existing calls will be allowed to finish. ContinueWhilePossible indicates that even if one task fails, all other task series should be continued until all possible jobs are completed either successfully or not. default_runtime_attributes.maxRetries: The maximum number of times a job can be retried if it fails in a way that is considered a retryable failure (like if a job gets dumped or the server goes down). write_to_cache: Do you want to write metadata about this workflow to the database to allow for future use of the cached files it might create? read_from_cache: Do you want to query the database to determine if portions of this workflow have already completed successfully, thus they do not need to be re-computed. 2.2.2 Runtime Defaults { &quot;default_runtime_attributes&quot;: { &quot;docker&quot;: &quot;ubuntu:latest&quot;, &quot;continueOnReturnCode&quot;: [4, 8, 15, 16, 23, 42] } } 2.2.3 Call Caching { &quot;write_to_cache&quot;: true, &quot;read_from_cache&quot;: true } 2.2.4 Workflow Failure Mode { &quot;workflow_failure_mode&quot;: &quot;ContinueWhilePossible&quot; } Values are: ContinueWhilePossible or NoNewCalls 2.2.5 Copying outputs Read more details here, but the ability to copy the workflow outputs to another location can be very useful for data management. { &quot;final_workflow_outputs_dir&quot;: &quot;/my/path/workflow/archive&quot;, &quot;use_relative_output_paths&quot;: false } If you want to collapse the directory structure, you can set use_relative_output_paths to true but if a file collision occurs Cromwell will stop and report the workflow as failed. "],["developing-testing-and-scaling-workflows.html", "Chapter 3 Developing, Testing and Scaling Workflows 3.1 Creating an Inputs template 3.2 Approaches to Testing and Development", " Chapter 3 Developing, Testing and Scaling Workflows 3.1 Creating an Inputs template There is a tool that is installed along with Cromwell called “womtool”. This tool is what does workflow validation through the Shiny app and fh.wdlR package. However, it has additional features that are not available unless you run it directly. There is specific documentation from the Broad about womtool that can give you more information about the parameters and features of this small tool. The most useful aspect to womtool is the ability to give it a new workflow you have written and have it give you a template input.json file for you to customize for use. To use it, log into Rhino and do the following: module load cromwell/84 java -jar $EBROOTCROMWELL/womtool.jar inputs myNewWorkflow.wdl &gt; input_template.json Now your template input information will be saved in the file input_template.json. 3.2 Approaches to Testing and Development 3.2.1 Start from a template Preferably one that executes cleanly - even if it’s Hello World. 3.2.2 Chose Software Modules Test interactively with software modules on the cluster to see what inputs are required, what parameters you want to specify, what outputs get created. 3.2.3 Add Tasks Define tasks, using modules, test data (truncated files, scatters of 1), run and test. 3.2.4 Scale Up Inputs Start to run workflow on full size data (scatters of 1), then start to scatter over several files, then scatter over entire datasets. 3.2.5 Prep for Other Platforms and Sharing Shift to docker containers instead of modules, ensure that all inputs are specified as files not directories!!, start to optimize compute resources required for tasks (how much memory does it really need, or is it going to need many CPU’s). - start small - start with truncated/downsampled data in the same formats but smaller! - add tasks one at a time and leverage call caching to test single additions quickly - start testing scatters with a scatter of one! 3.2.6 Testing Input Availability Validating a workflow only checks the formatting of the files but does not attempt to stage the inputs. To do that you might consider tricking Cromwell into localizing all external inputs first by creating a dummy task that runs first before any of your steps in your workflow. The inputs to this task need to be all externally obtained file inputs to any task in your workflow (not inputs to tasks that come from other WDL tasks!). Then, upon running this workflow, Cromwell will try to localize all the inputs for this first task, before running any future tasks. Then you have two options: - my inputs are small or local: just remove the input localizing task before re-running the workflow - my inputs are large or expensive to localize: specify the inputs to your workflow tasks as the outputs of this input localizing task by adding them as outputs. 3.2.7 Scaling Up, Moving to the Cloud Test with a scatter of many small files (not full size!) Begin optimizing computing resources for each task Dockerize single task environments Test locally on small data scatters of 1 to shift from modules to Dockerize Catch errors and adjust such as tools that rely on filesystems/directory structures which will break in the cloud Learn about the cloud you’ll be using what instance types exist and which are the best for you tasks/data how do the data need to be stored in an object store? how can you get permissions set up to access both the data and the compute resources? "],["wdl-at-fred-hutch.html", "Chapter 4 WDL at Fred Hutch", " Chapter 4 WDL at Fred Hutch Fred Hutch has begun supporting use of WDL workflows on their local SLURM cluster to support reproducible workflow development and testing locally while facilitating collaboration and cloud computing when needed. They have added some basic information about using WDL workflows and Cromwell in their documentation site. If you’re looking for Cromwell configuration information for a SLURM cluster (including wiring to convert Docker containers to Singularity prior to running jobs), you can find the diy-cromwell-server repo with instructions. We also have a repo containing some very basic WDL workflows meant primarily to test Cromwell servers and for new users to begin to learn how WDL workflows can be structured. The Fred Hutch Data Science Lab has developed an introductory guide for Fred Hutch staff to get set up to use Cromwell as it is configured in the above repo, though the content may be useful for others intending to do something similar at their institution. This Cromwell configuration is made simpler for use by staff via the fh.wdlR R package, a convenience package that wraps the Cromwell API calls and does some basic parsing of the metadata suitable for basic workflows. Also, there is a Shiny app here that leverages fh.wdlR and can be run on your local machine if desired to serve as a useful interface with your Cromwell server. On all these resources, we welcome your input on the various GitHub repo’s that drive them all. "],["about-the-authors.html", "About the Authors", " About the Authors     Credits Names Pedagogy Content Author(s) Amy Paguirigan Content Editor(s)/Reviewer(s) Checked your content Acknowledgments Gave small assistance to content but not to the level of consulting Technical Template Publishing Engineers Candace Savonen, Carrie Wright Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Candace Savonen Package Developers (ottrpal) Candace Savonen, John Muschelli, Carrie Wright   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.0.2 (2020-06-22) ## os Ubuntu 20.04.3 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2022-12-28 ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date lib source ## assertthat 0.2.1 2019-03-21 [1] RSPM (R 4.0.3) ## bookdown 0.24 2022-02-15 [1] Github (rstudio/bookdown@88bc4ea) ## callr 3.4.4 2020-09-07 [1] RSPM (R 4.0.2) ## cli 2.0.2 2020-02-28 [1] RSPM (R 4.0.0) ## crayon 1.3.4 2017-09-16 [1] RSPM (R 4.0.0) ## desc 1.2.0 2018-05-01 [1] RSPM (R 4.0.3) ## devtools 2.3.2 2020-09-18 [1] RSPM (R 4.0.3) ## digest 0.6.25 2020-02-23 [1] RSPM (R 4.0.0) ## ellipsis 0.3.1 2020-05-15 [1] RSPM (R 4.0.3) ## evaluate 0.14 2019-05-28 [1] RSPM (R 4.0.3) ## fansi 0.4.1 2020-01-08 [1] RSPM (R 4.0.0) ## fs 1.5.0 2020-07-31 [1] RSPM (R 4.0.3) ## glue 1.6.1 2022-01-22 [1] CRAN (R 4.0.2) ## hms 0.5.3 2020-01-08 [1] RSPM (R 4.0.0) ## htmltools 0.5.0 2020-06-16 [1] RSPM (R 4.0.1) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.0.2) ## knitr 1.33 2022-02-15 [1] Github (yihui/knitr@a1052d1) ## lifecycle 1.0.0 2021-02-15 [1] CRAN (R 4.0.2) ## magrittr 2.0.2 2022-01-26 [1] CRAN (R 4.0.2) ## memoise 1.1.0 2017-04-21 [1] RSPM (R 4.0.0) ## ottrpal 0.1.2 2022-02-15 [1] Github (jhudsl/ottrpal@1018848) ## pillar 1.4.6 2020-07-10 [1] RSPM (R 4.0.2) ## pkgbuild 1.1.0 2020-07-13 [1] RSPM (R 4.0.2) ## pkgconfig 2.0.3 2019-09-22 [1] RSPM (R 4.0.3) ## pkgload 1.1.0 2020-05-29 [1] RSPM (R 4.0.3) ## prettyunits 1.1.1 2020-01-24 [1] RSPM (R 4.0.3) ## processx 3.4.4 2020-09-03 [1] RSPM (R 4.0.2) ## ps 1.3.4 2020-08-11 [1] RSPM (R 4.0.2) ## purrr 0.3.4 2020-04-17 [1] RSPM (R 4.0.3) ## R6 2.4.1 2019-11-12 [1] RSPM (R 4.0.0) ## readr 1.4.0 2020-10-05 [1] RSPM (R 4.0.2) ## remotes 2.2.0 2020-07-21 [1] RSPM (R 4.0.3) ## rlang 0.4.10 2022-02-15 [1] Github (r-lib/rlang@f0c9be5) ## rmarkdown 2.10 2022-02-15 [1] Github (rstudio/rmarkdown@02d3c25) ## rprojroot 2.0.2 2020-11-15 [1] CRAN (R 4.0.2) ## sessioninfo 1.1.1 2018-11-05 [1] RSPM (R 4.0.3) ## stringi 1.5.3 2020-09-09 [1] RSPM (R 4.0.3) ## stringr 1.4.0 2019-02-10 [1] RSPM (R 4.0.3) ## testthat 3.0.1 2022-02-15 [1] Github (R-lib/testthat@e99155a) ## tibble 3.0.3 2020-07-10 [1] RSPM (R 4.0.2) ## usethis 2.1.5.9000 2022-02-15 [1] Github (r-lib/usethis@57b109a) ## vctrs 0.3.4 2020-08-29 [1] RSPM (R 4.0.2) ## withr 2.3.0 2020-09-22 [1] RSPM (R 4.0.2) ## xfun 0.26 2022-02-15 [1] Github (yihui/xfun@74c2a66) ## yaml 2.2.1 2020-02-01 [1] RSPM (R 4.0.3) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
